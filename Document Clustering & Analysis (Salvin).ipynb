{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba385b0a-d32f-4b5c-b007-cefa2d389bb3",
   "metadata": {},
   "source": [
    "# Document Clustering & Analysis using Custom K-Means\n",
    "This **Jupyter Notebook** looks a subset of the Newsgroup dataset. The **subset** for this dataset includes 2,500 documents, each belonging to one of **5 categories** which are: Windows (0), Crypt (1), Christian (2), Hockey (3), Forsale (4). The documents are represented by 9328 terms (stems). \n",
    "\n",
    "The **vocabulary** for the dataset is given in the file \"terms.txt\" and the **term-by-document** matrix is given in \"matrix.txt\". The actual category labels for the document is provided in the file \"classes.txt\". The **goal** of this lab is to perform clustering on the documents and compare the clusters to the actual categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab748407-4170-4e37-939c-d9b945cab643",
   "metadata": {},
   "source": [
    "### Importing Libraries\n",
    "We start by importing the following libraries:\n",
    "- **Pandas:** a powerful open-source data analysis and manipulation library for Python. It provides data structures and functions for efficiently working with structured data\n",
    "- **NumPy:** a open source Python library that's widely used in science and engineering. The NumPy library contains multidimensional array data structures, such as the homogeneous, N-dimensional ndarray\n",
    "- **Sklearn:**  a free and open-source machine learning library for the Python programming language\n",
    "- **kMeans:** this is a python script which contains the functions that are going to be used to calculate distances and etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b667613-8afb-4a58-adb2-c8645ae73533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from kMeans import *\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import completeness_score, homogeneity_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8015b0-734f-4748-a540-99265ea11b7d",
   "metadata": {},
   "source": [
    "## 1 |  Cosine Distance Function\n",
    "We start off by creating the cosine distance function. We create our own distance function, where instead of using Euclidean distance, we use Cosine Similarity. This is the distance function that will be used to pass to the KMeans function in the Python Script. We use a written version that computes the Cosine Similarity between two n-dimensional vectors and returns the inverse as the distance between the vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5318000-3de4-4f0a-a86e-ae6c3a40bb6e",
   "metadata": {},
   "source": [
    "### Changes to the KMeans.py File\n",
    "Here is the following change that was made to the `kMeans.py` file where the Cosine Distance function was added with the following logic:\n",
    "```\n",
    "def distCosine(vecA, vecB):\n",
    "    vector_A = np.array(vecA)\n",
    "    vector_B = np.array(vecB)\n",
    "\n",
    "    dot_product = np.dot(vector_A, vector_B)\n",
    "    magnitude_A = np.linalg.norm(vector_A)\n",
    "    magnitude_B = np.linalg.norm(vector_B)\n",
    "\n",
    "    if magnitude_A == 0 or magnitude_B == 0:\n",
    "        return 1.0\n",
    "\n",
    "    cosine_similarity = dot_product / (magnitude_A * magnitude_B)\n",
    "    cosine_distance = 1 - cosine_similarity\n",
    "\n",
    "    return cosine_distance\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38f6bd2-8a86-4749-9311-ebbde23e3b31",
   "metadata": {},
   "source": [
    "## 2 | Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd74e7a9",
   "metadata": {},
   "source": [
    "Next up, we perform the data preprocessing. We start off by loading the data sets and then perform the following:\n",
    "- [x] convert the data into a numpy array and and then transpose it to get the term-document matrix \n",
    "- [x] split the data set into training and testing data sets, where 20% of the dataset is saved for testning\n",
    "- [x] perform the tf-idf transformation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8424a03e",
   "metadata": {},
   "source": [
    "### The Matrix Data from `matrix.txt`\n",
    "We start off by loading the matrix.txt using `np.loadtxt()` and the delimiter is set to `,` because all the numbers are separated by commas. Once the numpy arrays have been loaded into the variable `data_matrix`, what we end up with the term-document matrix. Since we're going to be perform clustering, we perform the transpose by using `.T` which will give us the document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38c03e5f-6868-48ed-805e-191de8687ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the matrix data and transposing it\n",
    "data_matrix = np.loadtxt(\"matrix.txt\", delimiter=\",\")\n",
    "data_matrix = data_matrix.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762f3aad",
   "metadata": {},
   "source": [
    "### Splitting of the Dataset\n",
    "We then use the document-term matrix to split the data into training and testing dataset. 20% of the dataset is reserved for training and 80% for training. We use `random_state` to ensure that we're performing a randomized split of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "991f0d7f-dd2d-4119-9f9d-67a1d9c368b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset into training and testing dataset\n",
    "train_data, test_data = train_test_split(data_matrix, test_size=0.2, random_state=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2807d5",
   "metadata": {},
   "source": [
    "### The TF-IDF Transformation\n",
    "Lastly, we initialize the Tfidf transformer which is used to compute the TF-IDF representation of term-document matrices. We apply two tranformations, which is the term frequency and the inverse document frequency. We perform this on both the testing and training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46b61349-d274-4970-90bf-e10b3f42927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing the TF-IDF transformation\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "train_data_tfidf = tfidf_transformer.fit_transform(train_data).toarray()\n",
    "test_data_tfidf = tfidf_transformer.transform(test_data).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183cf28e-9aac-40e5-8480-917ac38ad63c",
   "metadata": {},
   "source": [
    "## 3 | Performing KMeans Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aebf7b",
   "metadata": {},
   "source": [
    "Next up, we peform the KMeans Clustering on the tranformed training data. This is to conduct a analysis of the clusters by examining the top features in each cluster to identify patterns in the data. \n",
    "\n",
    "For this, we use a for loop to display the top terms in each cluster, sorting them by their average TF-IDF weight from the cluster centroid. We also use the for loop to display the data about each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bad58be",
   "metadata": {},
   "source": [
    "### Opening the Text File\n",
    "We open the `terms.txt` file in read mode, where the program reads its contents. We then use list comprehension iterate over each line in the file object and create a list of words and store them in the variable called `vocabulary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e4d4958-c0d9-4285-bcef-fbfc84deca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('terms.txt', 'r') as file:\n",
    "    vocabulary = [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b65c40",
   "metadata": {},
   "source": [
    "### Different Vaues of K for Performing Clustering\n",
    "Starting off with a for-loop, we iterate through the K-values of 4 and 5. We run the `kMeans.py` script and suppress the print statements by redirecting the the standard output `stdout` to `os.devnull`. We then perform the k-means clustering and show the results of the clustering. Here are the next following steps:\n",
    "\n",
    "- [x] Iterate though the clusters corresponding to the values of k and calculate the centroids\n",
    "- [x] Create print statements that show the mean TF-IDF weights of that term, cluster DF count and cluster DF percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "695a837d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running K-means with k = 4\n",
      "Cluster 1 Size = 793\n",
      "Term           Mean TF-IDF DF    % of Docs \n",
      "---------------------------------------------\n",
      "window         0.06        302   38.08     \n",
      "sale           0.04        236   29.76     \n",
      "file           0.03        161   20.30     \n",
      "subject        0.03        793   100.00    \n",
      "driver         0.02        95    11.98     \n",
      "\n",
      "\n",
      "Cluster 2 Size = 399\n",
      "Term           Mean TF-IDF DF    % of Docs \n",
      "---------------------------------------------\n",
      "game           0.07        206   51.63     \n",
      "team           0.05        184   46.12     \n",
      "plai           0.04        162   40.60     \n",
      "hockei         0.04        171   42.86     \n",
      "go             0.03        176   44.11     \n",
      "\n",
      "\n",
      "Cluster 3 Size = 408\n",
      "Term           Mean TF-IDF DF    % of Docs \n",
      "---------------------------------------------\n",
      "god            0.07        223   54.66     \n",
      "christian      0.05        182   44.61     \n",
      "sin            0.03        84    20.59     \n",
      "jesu           0.03        111   27.21     \n",
      "church         0.03        97    23.77     \n",
      "\n",
      "\n",
      "Cluster 4 Size = 400\n",
      "Term           Mean TF-IDF DF    % of Docs \n",
      "---------------------------------------------\n",
      "kei            0.07        196   49.00     \n",
      "chip           0.06        167   41.75     \n",
      "encrypt        0.05        186   46.50     \n",
      "clipper        0.05        192   48.00     \n",
      "govern         0.04        141   35.25     \n",
      "\n",
      "\n",
      "Running K-means with k = 5\n",
      "Cluster 1 Size = 400\n",
      "Term           Mean TF-IDF DF    % of Docs \n",
      "---------------------------------------------\n",
      "window         0.11        288   72.00     \n",
      "file           0.06        156   39.00     \n",
      "driver         0.05        90    22.50     \n",
      "do             0.04        130   32.50     \n",
      "program        0.03        108   27.00     \n",
      "\n",
      "\n",
      "Cluster 2 Size = 391\n",
      "Term           Mean TF-IDF DF    % of Docs \n",
      "---------------------------------------------\n",
      "kei            0.08        196   50.13     \n",
      "chip           0.06        166   42.46     \n",
      "encrypt        0.05        187   47.83     \n",
      "clipper        0.05        192   49.10     \n",
      "govern         0.04        140   35.81     \n",
      "\n",
      "\n",
      "Cluster 3 Size = 396\n",
      "Term           Mean TF-IDF DF    % of Docs \n",
      "---------------------------------------------\n",
      "game           0.07        203   51.26     \n",
      "team           0.05        184   46.46     \n",
      "plai           0.04        162   40.91     \n",
      "hockei         0.04        169   42.68     \n",
      "go             0.03        176   44.44     \n",
      "\n",
      "\n",
      "Cluster 4 Size = 417\n",
      "Term           Mean TF-IDF DF    % of Docs \n",
      "---------------------------------------------\n",
      "god            0.07        223   53.48     \n",
      "christian      0.05        182   43.65     \n",
      "sin            0.03        84    20.14     \n",
      "jesu           0.03        111   26.62     \n",
      "church         0.03        97    23.26     \n",
      "\n",
      "\n",
      "Cluster 5 Size = 396\n",
      "Term           Mean TF-IDF DF    % of Docs \n",
      "---------------------------------------------\n",
      "sale           0.07        232   58.59     \n",
      "offer          0.03        113   28.54     \n",
      "email          0.03        144   36.36     \n",
      "drive          0.03        57    14.39     \n",
      "ship           0.03        98    24.75     \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for loop through the values of k\n",
    "for k in range(4, 6):\n",
    "    print(f\"Running K-means with k = {k}\")\n",
    "    \n",
    "    # suppression of print statements\n",
    "    stdout = sys.stdout \n",
    "    sys.stdout = open(os.devnull, 'w')\n",
    "    \n",
    "    # running the K-means clustering algorithm using the kMeans.py file\n",
    "    centroids, cluster_assment = kMeans(train_data_tfidf, k, distMeas=distCosine, createCent=randCent)\n",
    "    \n",
    "    # end of suppression as we restore the standard output\n",
    "    sys.stdout.close()\n",
    "    sys.stdout = stdout\n",
    "\n",
    "    # extracting the number of documents and features from the shape of the matrix\n",
    "    n_docs, n_terms = train_data_tfidf.shape\n",
    "    \n",
    "    # to display the top 5 items\n",
    "    top_n = 5\n",
    "\n",
    "    # iterating through the clusters\n",
    "    for cluster in range(k):\n",
    "        \n",
    "        # retrieving the indices, tf-idf vectors of documents and number of documents\n",
    "        cluster_indices = np.nonzero(cluster_assment[:, 0] == cluster)[0]\n",
    "        cluster_docs = train_data_tfidf[cluster_indices]\n",
    "        cluster_size = len(cluster_docs)\n",
    "\n",
    "        # calculating the mean tf-idf vector and represents the centroid of the cluster\n",
    "        avg_tfidf = centroids[cluster]\n",
    "\n",
    "        # identifying the top terms in the cluster by sorting the term indices based on tf-idf weights\n",
    "        top_indices = np.argsort(avg_tfidf)[::-1][:top_n]\n",
    "        top_terms = [(vocabulary[i], avg_tfidf[i]) for i in top_indices]\n",
    "\n",
    "        # printing the top 5 terms in each cluster as well as other details\n",
    "        print(f\"Cluster {cluster + 1} Size = {cluster_size}\")\n",
    "        print(f\"{'Term':<15}{'Mean TF-IDF':<12}{'DF':<6}{'% of Docs':<10}\")\n",
    "        print(\"-\" * 45)\n",
    "\n",
    "        # printing the index of the term, how many documents in the cluster contain the term and percentage\n",
    "        for term, weight in top_terms:\n",
    "            term_index = vocabulary.index(term)\n",
    "            term_doc_count = np.sum(cluster_docs[:, term_index] > 0)\n",
    "            term_doc_percentage = (term_doc_count / cluster_size) * 100\n",
    "            print(f\"{term:<15}{weight:<12.2f}{term_doc_count:<6}{term_doc_percentage:<10.2f}\")\n",
    "\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5f3225",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "It can be seen that the best clustering result is when `k=5`. This is because when `k=5`, we can see that the clusters have produced easy to interpret theme or a common topic compared to when `k=4`.\n",
    "\n",
    "When we look at the clusters, for example at `Cluster 2`, it can be seen that the top terms are `god`, `christian`, `jesus`, `sin` and `church`. This means that the cluster is about topics related to Christianity. Other similar themes are discovered in other clusters, for example in `Cluster 1`, the topic might be about Cryptography because the terms are `encrypt` or `kei`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5ad219",
   "metadata": {},
   "source": [
    "## 4 | Evaluating Cluster Quality\n",
    "Now, we evaluate the quality of the clusters. Using the clusters we produced, we assess how well the clusters match the actual categories by calculating the Completeness and Homogeneity Scores. Here are the following things to do:\n",
    "- [x] determining the representative label based on a majority vote of the true labels on the training documents\n",
    "- [x] calculating the completeness and homoegeneity scores, where completeness indicates whether all documents as assigned to the same cluster, and homogeneity determines whether each cluster contains only documents that belong to a single cateogory\n",
    "- [x] we perform this analysis using the best clustering result that was identified in the previous part "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b6786e",
   "metadata": {},
   "source": [
    "### Loading Category Labels\n",
    "Here, we load the `classes.txt` file which contains all the labels, and the split them into test and training data, where 20% of the data is dedicated towards testing and 80% towards training. We use `random_state` to ensure the splitting is randomized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3874a616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieving the true cateogry labels\n",
    "true_labels = np.loadtxt('classes.txt', dtype=int, comments='%', usecols=1)\n",
    "\n",
    "# splitting the labels into training and testing data\n",
    "train_labels, test_labels = train_test_split(true_labels, test_size=0.2, random_state=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8636784",
   "metadata": {},
   "source": [
    "### Mapping Clusters to Representative Label\n",
    "Next, we initailize a dictionary to map each cluster to its representative label. For each cluster, we identify the documents assigned to that cluster, retrieve the tue label, and determine the most frequent label, which is then assigned to that cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47b2b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intializing a dictionary\n",
    "cluster_to_label = {}\n",
    "\n",
    "# iterating using for loop through each cluster\n",
    "for cluster in range(k):\n",
    "    \n",
    "    # retreiving the indices of the cluster\n",
    "    cluster_indices = np.nonzero(cluster_assment[:, 0] == cluster)[0]\n",
    "    \n",
    "    # retreiving the true labels\n",
    "    cluster_labels = train_labels[cluster_indices]\n",
    "    \n",
    "    # determining the most common label\n",
    "    most_common_label = Counter(cluster_labels).most_common(1)[0][0]\n",
    "    cluster_to_label[cluster] = most_common_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eabf492b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping the cluster assignments to the label\n",
    "predicted_labels = np.array([cluster_to_label[int(cluster)] for cluster in cluster_assment[:, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4dfb3b",
   "metadata": {},
   "source": [
    "### Computing the Completeness & Homogeneity Scores\n",
    "Lastly, we compute the completeness and homogeneity scores using the training labels and the predicted labels, and then display the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6527b1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completeness Score: 0.8493\n",
      "Homogeneity Score: 0.8492\n"
     ]
    }
   ],
   "source": [
    "# Compute scores\n",
    "completeness = completeness_score(train_labels, predicted_labels)\n",
    "homogeneity = homogeneity_score(train_labels, predicted_labels)\n",
    "\n",
    "print(f\"Completeness Score: {completeness:.4f}\")\n",
    "print(f\"Homogeneity Score: {homogeneity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c27a081",
   "metadata": {},
   "source": [
    "## 5 | Classifying Test Documents\n",
    "Using the final cluster centroids from the best K-means clustering, we classify each document in the 20% test set by assinging it to the closest cluster based on the Cosine similarity. Here are the following things to do:\n",
    "- [x] Calculating the cosine similarity for each document in the test set\n",
    "- [x] Assinging cluster labels to test documents, where for each document, we assign it to the cluster with the highest cosine similarity score\n",
    "- [x] displaying the results of the classification by showinf the assigned cluster label and the cosine similairty score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397293d1",
   "metadata": {},
   "source": [
    "### Calculating Cosine Simialirty & Assignin Cluster Labels\n",
    "We start off here by classifying each document in the test set by calculating the cosine distance between the documents TF-IDF vector and each cluster centroid. The document is then assigned to the cluster with the smallest distance, which where the cosine similarity distance calculation comes into play. The smallest distance would be the highest cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "326d1708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing document classifications\n",
    "test_results = []\n",
    "\n",
    "# classifying each document\n",
    "for idx, doc_vector in enumerate(test_data_tfidf):\n",
    "    \n",
    "    # computing cosine distances\n",
    "    distances = [distCosine(doc_vector, centroid) for centroid in centroids]\n",
    "    \n",
    "    # finding the closes centroid\n",
    "    best_cluster = np.argmin(distances)\n",
    "    best_similarity = 1 - distances[best_cluster]  # Convert distance back to similarity\n",
    "    \n",
    "    # mapping the cluster to its labels\n",
    "    pseudo_label = cluster_to_label[best_cluster]\n",
    "    \n",
    "    # storing the results\n",
    "    test_results.append((idx, best_cluster, pseudo_label, best_similarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c980f3bc",
   "metadata": {},
   "source": [
    "### Displaying the Results\n",
    "Lastly, for each test document, we output the assigned cluster label and the cosine similarity score, as well the ID of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5415976b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Document Classification Results:\n",
      "Doc ID | Assigned Cluster | Label | Cosine Similarity\n",
      "     0 |                1 |            1 | 0.3038\n",
      "     1 |                1 |            1 | 0.1477\n",
      "     2 |                0 |            0 | 0.2263\n",
      "     3 |                1 |            1 | 0.1487\n",
      "     4 |                3 |            2 | 0.3291\n",
      "     5 |                2 |            3 | 0.3100\n",
      "     6 |                3 |            2 | 0.3528\n",
      "     7 |                3 |            2 | 0.1691\n",
      "     8 |                2 |            3 | 0.2375\n",
      "     9 |                3 |            2 | 0.3162\n",
      "    10 |                2 |            3 | 0.2781\n",
      "    11 |                4 |            4 | 0.0574\n",
      "    12 |                2 |            3 | 0.2731\n",
      "    13 |                2 |            3 | 0.2118\n",
      "    14 |                3 |            2 | 0.3059\n",
      "    15 |                3 |            2 | 0.0761\n",
      "    16 |                2 |            3 | 0.1913\n",
      "    17 |                3 |            2 | 0.3217\n",
      "    18 |                1 |            1 | 0.2535\n",
      "    19 |                4 |            4 | 0.1977\n",
      "    20 |                0 |            0 | 0.1194\n",
      "    21 |                0 |            0 | 0.1515\n",
      "    22 |                1 |            1 | 0.1541\n",
      "    23 |                3 |            2 | 0.3014\n",
      "    24 |                0 |            0 | 0.2575\n",
      "    25 |                2 |            3 | 0.2267\n",
      "    26 |                3 |            2 | 0.2462\n",
      "    27 |                0 |            0 | 0.2744\n",
      "    28 |                4 |            4 | 0.4060\n",
      "    29 |                1 |            1 | 0.1556\n",
      "    30 |                1 |            1 | 0.0798\n",
      "    31 |                4 |            4 | 0.1202\n",
      "    32 |                2 |            3 | 0.3176\n",
      "    33 |                4 |            4 | 0.1655\n",
      "    34 |                3 |            2 | 0.2696\n",
      "    35 |                0 |            0 | 0.1980\n",
      "    36 |                4 |            4 | 0.0267\n",
      "    37 |                2 |            3 | 0.2054\n",
      "    38 |                2 |            3 | 0.1072\n",
      "    39 |                4 |            4 | 0.1121\n",
      "    40 |                3 |            2 | 0.2223\n",
      "    41 |                0 |            0 | 0.1582\n",
      "    42 |                4 |            4 | 0.1946\n",
      "    43 |                1 |            1 | 0.1095\n",
      "    44 |                1 |            1 | 0.1358\n",
      "    45 |                1 |            1 | 0.2533\n",
      "    46 |                3 |            2 | 0.1996\n",
      "    47 |                4 |            4 | 0.0953\n",
      "    48 |                1 |            1 | 0.4070\n",
      "    49 |                0 |            0 | 0.2901\n",
      "    50 |                3 |            2 | 0.1711\n",
      "    51 |                1 |            1 | 0.1961\n",
      "    52 |                0 |            0 | 0.2526\n",
      "    53 |                2 |            3 | 0.0734\n",
      "    54 |                3 |            2 | 0.1939\n",
      "    55 |                2 |            3 | 0.2175\n",
      "    56 |                1 |            1 | 0.1576\n",
      "    57 |                2 |            3 | 0.1043\n",
      "    58 |                3 |            2 | 0.2467\n",
      "    59 |                3 |            2 | 0.3294\n",
      "    60 |                0 |            0 | 0.1496\n",
      "    61 |                1 |            1 | 0.2497\n",
      "    62 |                2 |            3 | 0.1265\n",
      "    63 |                3 |            2 | 0.0565\n",
      "    64 |                0 |            0 | 0.1566\n",
      "    65 |                3 |            2 | 0.3427\n",
      "    66 |                3 |            2 | 0.2902\n",
      "    67 |                2 |            3 | 0.1498\n",
      "    68 |                0 |            0 | 0.2756\n",
      "    69 |                4 |            4 | 0.1466\n",
      "    70 |                1 |            1 | 0.1346\n",
      "    71 |                2 |            3 | 0.2597\n",
      "    72 |                0 |            0 | 0.1590\n",
      "    73 |                0 |            0 | 0.1313\n",
      "    74 |                3 |            2 | 0.1435\n",
      "    75 |                4 |            4 | 0.2047\n",
      "    76 |                2 |            3 | 0.1719\n",
      "    77 |                2 |            3 | 0.2307\n",
      "    78 |                3 |            2 | 0.3573\n",
      "    79 |                2 |            3 | 0.2303\n",
      "    80 |                3 |            2 | 0.1291\n",
      "    81 |                0 |            0 | 0.0178\n",
      "    82 |                4 |            4 | 0.0674\n",
      "    83 |                1 |            1 | 0.3793\n",
      "    84 |                0 |            0 | 0.1908\n",
      "    85 |                4 |            4 | 0.3352\n",
      "    86 |                2 |            3 | 0.2499\n",
      "    87 |                1 |            1 | 0.2325\n",
      "    88 |                3 |            2 | 0.2554\n",
      "    89 |                1 |            1 | 0.2892\n",
      "    90 |                3 |            2 | 0.1348\n",
      "    91 |                3 |            2 | 0.2555\n",
      "    92 |                3 |            2 | 0.0609\n",
      "    93 |                0 |            0 | 0.1521\n",
      "    94 |                0 |            0 | 0.3599\n",
      "    95 |                1 |            1 | 0.1901\n",
      "    96 |                0 |            0 | 0.3104\n",
      "    97 |                3 |            2 | 0.2335\n",
      "    98 |                2 |            3 | 0.2894\n",
      "    99 |                1 |            1 | 0.2387\n",
      "   100 |                4 |            4 | 0.0775\n",
      "   101 |                2 |            3 | 0.1145\n",
      "   102 |                2 |            3 | 0.2491\n",
      "   103 |                0 |            0 | 0.1619\n",
      "   104 |                3 |            2 | 0.2106\n",
      "   105 |                4 |            4 | 0.2162\n",
      "   106 |                2 |            3 | 0.2260\n",
      "   107 |                3 |            2 | 0.2419\n",
      "   108 |                1 |            1 | 0.2639\n",
      "   109 |                4 |            4 | 0.1621\n",
      "   110 |                2 |            3 | 0.2720\n",
      "   111 |                1 |            1 | 0.1639\n",
      "   112 |                2 |            3 | 0.2758\n",
      "   113 |                3 |            2 | 0.2145\n",
      "   114 |                4 |            4 | 0.0395\n",
      "   115 |                3 |            2 | 0.2585\n",
      "   116 |                3 |            2 | 0.3711\n",
      "   117 |                2 |            3 | 0.1744\n",
      "   118 |                4 |            4 | 0.1417\n",
      "   119 |                2 |            3 | 0.1430\n",
      "   120 |                1 |            1 | 0.2938\n",
      "   121 |                1 |            1 | 0.3072\n",
      "   122 |                3 |            2 | 0.1110\n",
      "   123 |                1 |            1 | 0.3343\n",
      "   124 |                4 |            4 | 0.1617\n",
      "   125 |                1 |            1 | 0.2043\n",
      "   126 |                2 |            3 | 0.1121\n",
      "   127 |                3 |            2 | 0.2953\n",
      "   128 |                4 |            4 | 0.1822\n",
      "   129 |                1 |            1 | 0.3289\n",
      "   130 |                4 |            4 | 0.1305\n",
      "   131 |                0 |            0 | 0.3059\n",
      "   132 |                2 |            3 | 0.1678\n",
      "   133 |                3 |            2 | 0.1311\n",
      "   134 |                4 |            4 | 0.1814\n",
      "   135 |                4 |            4 | 0.1763\n",
      "   136 |                1 |            1 | 0.1004\n",
      "   137 |                3 |            2 | 0.1858\n",
      "   138 |                3 |            2 | 0.1771\n",
      "   139 |                2 |            3 | 0.1983\n",
      "   140 |                2 |            3 | 0.2557\n",
      "   141 |                2 |            3 | 0.0992\n",
      "   142 |                2 |            3 | 0.1621\n",
      "   143 |                0 |            0 | 0.1932\n",
      "   144 |                2 |            3 | 0.1556\n",
      "   145 |                1 |            1 | 0.2962\n",
      "   146 |                2 |            3 | 0.1076\n",
      "   147 |                2 |            3 | 0.2731\n",
      "   148 |                4 |            4 | 0.2394\n",
      "   149 |                4 |            4 | 0.1262\n",
      "   150 |                2 |            3 | 0.1093\n",
      "   151 |                1 |            1 | 0.3013\n",
      "   152 |                2 |            3 | 0.2206\n",
      "   153 |                0 |            0 | 0.2057\n",
      "   154 |                4 |            4 | 0.3156\n",
      "   155 |                0 |            0 | 0.2409\n",
      "   156 |                2 |            3 | 0.2396\n",
      "   157 |                1 |            1 | 0.1616\n",
      "   158 |                0 |            0 | 0.0671\n",
      "   159 |                1 |            1 | 0.1793\n",
      "   160 |                2 |            3 | 0.1695\n",
      "   161 |                2 |            3 | 0.3944\n",
      "   162 |                1 |            1 | 0.5056\n",
      "   163 |                2 |            3 | 0.2516\n",
      "   164 |                3 |            2 | 0.2637\n",
      "   165 |                0 |            0 | 0.2003\n",
      "   166 |                0 |            0 | 0.2098\n",
      "   167 |                4 |            4 | 0.1176\n",
      "   168 |                0 |            0 | 0.1045\n",
      "   169 |                1 |            1 | 0.2326\n",
      "   170 |                0 |            0 | 0.1000\n",
      "   171 |                0 |            0 | 0.0926\n",
      "   172 |                4 |            4 | 0.2055\n",
      "   173 |                0 |            0 | 0.3668\n",
      "   174 |                4 |            4 | 0.0681\n",
      "   175 |                4 |            4 | 0.1683\n",
      "   176 |                1 |            1 | 0.3009\n",
      "   177 |                3 |            2 | 0.1548\n",
      "   178 |                4 |            4 | 0.2323\n",
      "   179 |                4 |            4 | 0.3038\n",
      "   180 |                3 |            2 | 0.0704\n",
      "   181 |                4 |            4 | 0.1015\n",
      "   182 |                0 |            0 | 0.1592\n",
      "   183 |                4 |            4 | 0.1598\n",
      "   184 |                0 |            0 | 0.1519\n",
      "   185 |                4 |            4 | 0.2016\n",
      "   186 |                0 |            0 | 0.2159\n",
      "   187 |                0 |            0 | 0.2882\n",
      "   188 |                4 |            4 | 0.1187\n",
      "   189 |                0 |            0 | 0.2826\n",
      "   190 |                0 |            0 | 0.1499\n",
      "   191 |                1 |            1 | 0.0860\n",
      "   192 |                3 |            2 | 0.2676\n",
      "   193 |                2 |            3 | 0.2836\n",
      "   194 |                3 |            2 | 0.2929\n",
      "   195 |                4 |            4 | 0.1465\n",
      "   196 |                2 |            3 | 0.1834\n",
      "   197 |                4 |            4 | 0.1894\n",
      "   198 |                2 |            3 | 0.2358\n",
      "   199 |                2 |            3 | 0.2335\n",
      "   200 |                2 |            3 | 0.2492\n",
      "   201 |                3 |            2 | 0.1433\n",
      "   202 |                4 |            4 | 0.1307\n",
      "   203 |                0 |            0 | 0.3011\n",
      "   204 |                4 |            4 | 0.2004\n",
      "   205 |                3 |            2 | 0.3816\n",
      "   206 |                0 |            0 | 0.1623\n",
      "   207 |                0 |            0 | 0.2518\n",
      "   208 |                1 |            1 | 0.2371\n",
      "   209 |                1 |            1 | 0.1789\n",
      "   210 |                1 |            1 | 0.2234\n",
      "   211 |                4 |            4 | 0.1935\n",
      "   212 |                2 |            3 | 0.2224\n",
      "   213 |                3 |            2 | 0.2341\n",
      "   214 |                3 |            2 | 0.4236\n",
      "   215 |                2 |            3 | 0.2129\n",
      "   216 |                2 |            3 | 0.1718\n",
      "   217 |                4 |            4 | 0.0671\n",
      "   218 |                1 |            1 | 0.3521\n",
      "   219 |                4 |            4 | 0.1466\n",
      "   220 |                1 |            1 | 0.2519\n",
      "   221 |                3 |            2 | 0.1755\n",
      "   222 |                3 |            2 | 0.1625\n",
      "   223 |                3 |            2 | 0.2552\n",
      "   224 |                4 |            4 | 0.1649\n",
      "   225 |                3 |            2 | 0.1140\n",
      "   226 |                1 |            1 | 0.0723\n",
      "   227 |                0 |            0 | 0.2044\n",
      "   228 |                1 |            1 | 0.1784\n",
      "   229 |                0 |            0 | 0.1479\n",
      "   230 |                3 |            2 | 0.2353\n",
      "   231 |                3 |            2 | 0.3532\n",
      "   232 |                4 |            4 | 0.1129\n",
      "   233 |                1 |            1 | 0.1701\n",
      "   234 |                0 |            0 | 0.2056\n",
      "   235 |                4 |            4 | 0.3006\n",
      "   236 |                2 |            3 | 0.2159\n",
      "   237 |                1 |            1 | 0.3459\n",
      "   238 |                0 |            0 | 0.2685\n",
      "   239 |                0 |            0 | 0.3630\n",
      "   240 |                1 |            1 | 0.3819\n",
      "   241 |                2 |            3 | 0.1749\n",
      "   242 |                0 |            0 | 0.2551\n",
      "   243 |                3 |            2 | 0.1928\n",
      "   244 |                4 |            4 | 0.1314\n",
      "   245 |                3 |            2 | 0.1453\n",
      "   246 |                0 |            0 | 0.1340\n",
      "   247 |                1 |            1 | 0.2482\n",
      "   248 |                2 |            3 | 0.1605\n",
      "   249 |                0 |            0 | 0.3939\n",
      "   250 |                4 |            4 | 0.1831\n",
      "   251 |                3 |            2 | 0.2286\n",
      "   252 |                2 |            3 | 0.2810\n",
      "   253 |                2 |            3 | 0.1565\n",
      "   254 |                4 |            4 | 0.1534\n",
      "   255 |                1 |            1 | 0.2253\n",
      "   256 |                1 |            1 | 0.1060\n",
      "   257 |                0 |            0 | 0.1976\n",
      "   258 |                3 |            2 | 0.2965\n",
      "   259 |                4 |            4 | 0.1716\n",
      "   260 |                2 |            3 | 0.3032\n",
      "   261 |                3 |            2 | 0.2132\n",
      "   262 |                2 |            3 | 0.0891\n",
      "   263 |                3 |            2 | 0.1493\n",
      "   264 |                4 |            4 | 0.2106\n",
      "   265 |                3 |            2 | 0.1093\n",
      "   266 |                0 |            0 | 0.1202\n",
      "   267 |                1 |            1 | 0.1495\n",
      "   268 |                3 |            2 | 0.3335\n",
      "   269 |                4 |            4 | 0.2318\n",
      "   270 |                1 |            1 | 0.3562\n",
      "   271 |                0 |            0 | 0.1262\n",
      "   272 |                2 |            3 | 0.2151\n",
      "   273 |                3 |            2 | 0.1883\n",
      "   274 |                1 |            1 | 0.0828\n",
      "   275 |                2 |            3 | 0.3501\n",
      "   276 |                1 |            1 | 0.1852\n",
      "   277 |                3 |            2 | 0.2074\n",
      "   278 |                4 |            4 | 0.1019\n",
      "   279 |                3 |            2 | 0.0867\n",
      "   280 |                0 |            0 | 0.2166\n",
      "   281 |                4 |            4 | 0.1348\n",
      "   282 |                1 |            1 | 0.1332\n",
      "   283 |                2 |            3 | 0.0651\n",
      "   284 |                2 |            3 | 0.2147\n",
      "   285 |                4 |            4 | 0.1082\n",
      "   286 |                4 |            4 | 0.1330\n",
      "   287 |                0 |            0 | 0.1327\n",
      "   288 |                1 |            1 | 0.3195\n",
      "   289 |                2 |            3 | 0.1681\n",
      "   290 |                2 |            3 | 0.1432\n",
      "   291 |                0 |            0 | 0.1182\n",
      "   292 |                3 |            2 | 0.3575\n",
      "   293 |                3 |            2 | 0.1452\n",
      "   294 |                0 |            0 | 0.1029\n",
      "   295 |                3 |            2 | 0.1539\n",
      "   296 |                3 |            2 | 0.3025\n",
      "   297 |                2 |            3 | 0.2465\n",
      "   298 |                2 |            3 | 0.2678\n",
      "   299 |                2 |            3 | 0.1659\n",
      "   300 |                2 |            3 | 0.2046\n",
      "   301 |                4 |            4 | 0.1494\n",
      "   302 |                3 |            2 | 0.2082\n",
      "   303 |                3 |            2 | 0.3534\n",
      "   304 |                1 |            1 | 0.2996\n",
      "   305 |                4 |            4 | 0.1587\n",
      "   306 |                4 |            4 | 0.2041\n",
      "   307 |                4 |            4 | 0.1780\n",
      "   308 |                2 |            3 | 0.2182\n",
      "   309 |                4 |            4 | 0.1537\n",
      "   310 |                4 |            4 | 0.1538\n",
      "   311 |                3 |            2 | 0.1921\n",
      "   312 |                3 |            2 | 0.2336\n",
      "   313 |                2 |            3 | 0.4160\n",
      "   314 |                0 |            0 | 0.0934\n",
      "   315 |                0 |            0 | 0.2308\n",
      "   316 |                3 |            2 | 0.2745\n",
      "   317 |                4 |            4 | 0.2308\n",
      "   318 |                0 |            0 | 0.1683\n",
      "   319 |                1 |            1 | 0.4241\n",
      "   320 |                0 |            0 | 0.1429\n",
      "   321 |                1 |            1 | 0.2155\n",
      "   322 |                3 |            2 | 0.3372\n",
      "   323 |                4 |            4 | 0.2649\n",
      "   324 |                2 |            3 | 0.3318\n",
      "   325 |                0 |            0 | 0.1192\n",
      "   326 |                2 |            3 | 0.1468\n",
      "   327 |                3 |            2 | 0.1445\n",
      "   328 |                0 |            0 | 0.3018\n",
      "   329 |                2 |            3 | 0.1836\n",
      "   330 |                3 |            2 | 0.1800\n",
      "   331 |                3 |            2 | 0.2965\n",
      "   332 |                4 |            4 | 0.2115\n",
      "   333 |                3 |            2 | 0.3483\n",
      "   334 |                3 |            2 | 0.1287\n",
      "   335 |                2 |            3 | 0.2285\n",
      "   336 |                0 |            0 | 0.1613\n",
      "   337 |                0 |            0 | 0.1306\n",
      "   338 |                1 |            1 | 0.0562\n",
      "   339 |                4 |            4 | 0.1952\n",
      "   340 |                1 |            1 | 0.1855\n",
      "   341 |                2 |            3 | 0.2793\n",
      "   342 |                1 |            1 | 0.1067\n",
      "   343 |                0 |            0 | 0.1773\n",
      "   344 |                0 |            0 | 0.1594\n",
      "   345 |                3 |            2 | 0.2348\n",
      "   346 |                3 |            2 | 0.1646\n",
      "   347 |                2 |            3 | 0.1932\n",
      "   348 |                4 |            4 | 0.1436\n",
      "   349 |                2 |            3 | 0.1937\n",
      "   350 |                2 |            3 | 0.1787\n",
      "   351 |                1 |            1 | 0.1333\n",
      "   352 |                4 |            4 | 0.1977\n",
      "   353 |                4 |            4 | 0.1594\n",
      "   354 |                0 |            0 | 0.1617\n",
      "   355 |                1 |            1 | 0.2249\n",
      "   356 |                1 |            1 | 0.2976\n",
      "   357 |                0 |            0 | 0.1442\n",
      "   358 |                4 |            4 | 0.1910\n",
      "   359 |                2 |            3 | 0.1738\n",
      "   360 |                2 |            3 | 0.1626\n",
      "   361 |                1 |            1 | 0.2724\n",
      "   362 |                2 |            3 | 0.1218\n",
      "   363 |                4 |            4 | 0.1948\n",
      "   364 |                2 |            3 | 0.0770\n",
      "   365 |                4 |            4 | 0.1079\n",
      "   366 |                4 |            4 | 0.1616\n",
      "   367 |                0 |            0 | 0.2273\n",
      "   368 |                4 |            4 | 0.2484\n",
      "   369 |                1 |            1 | 0.0763\n",
      "   370 |                4 |            4 | 0.1978\n",
      "   371 |                0 |            0 | 0.1490\n",
      "   372 |                3 |            2 | 0.1931\n",
      "   373 |                0 |            0 | 0.1860\n",
      "   374 |                3 |            2 | 0.1109\n",
      "   375 |                1 |            1 | 0.4011\n",
      "   376 |                0 |            0 | 0.1111\n",
      "   377 |                1 |            1 | 0.1364\n",
      "   378 |                3 |            2 | 0.1606\n",
      "   379 |                4 |            4 | 0.0696\n",
      "   380 |                0 |            0 | 0.1728\n",
      "   381 |                0 |            0 | 0.0894\n",
      "   382 |                1 |            1 | 0.3990\n",
      "   383 |                3 |            2 | 0.3499\n",
      "   384 |                1 |            1 | 0.2424\n",
      "   385 |                0 |            0 | 0.3019\n",
      "   386 |                1 |            1 | 0.1543\n",
      "   387 |                3 |            2 | 0.1607\n",
      "   388 |                0 |            0 | 0.4037\n",
      "   389 |                2 |            3 | 0.2203\n",
      "   390 |                4 |            4 | 0.1683\n",
      "   391 |                3 |            2 | 0.0966\n",
      "   392 |                2 |            3 | 0.2798\n",
      "   393 |                2 |            3 | 0.1428\n",
      "   394 |                2 |            3 | 0.1378\n",
      "   395 |                3 |            2 | 0.1365\n",
      "   396 |                4 |            4 | 0.1819\n",
      "   397 |                3 |            2 | 0.2317\n",
      "   398 |                1 |            1 | 0.2742\n",
      "   399 |                0 |            0 | 0.4367\n",
      "   400 |                2 |            3 | 0.3377\n",
      "   401 |                1 |            1 | 0.1821\n",
      "   402 |                1 |            1 | 0.1995\n",
      "   403 |                1 |            1 | 0.2495\n",
      "   404 |                3 |            2 | 0.1601\n",
      "   405 |                0 |            0 | 0.1166\n",
      "   406 |                4 |            4 | 0.1800\n",
      "   407 |                0 |            0 | 0.2365\n",
      "   408 |                4 |            4 | 0.2314\n",
      "   409 |                1 |            1 | 0.2939\n",
      "   410 |                3 |            2 | 0.3893\n",
      "   411 |                4 |            4 | 0.0497\n",
      "   412 |                4 |            4 | 0.2437\n",
      "   413 |                0 |            0 | 0.2751\n",
      "   414 |                3 |            2 | 0.3133\n",
      "   415 |                4 |            4 | 0.1924\n",
      "   416 |                4 |            4 | 0.1382\n",
      "   417 |                4 |            4 | 0.3397\n",
      "   418 |                0 |            0 | 0.2520\n",
      "   419 |                4 |            4 | 0.2604\n",
      "   420 |                3 |            2 | 0.1158\n",
      "   421 |                3 |            2 | 0.2360\n",
      "   422 |                2 |            3 | 0.2558\n",
      "   423 |                4 |            4 | 0.2396\n",
      "   424 |                4 |            4 | 0.1307\n",
      "   425 |                0 |            0 | 0.2315\n",
      "   426 |                3 |            2 | 0.2828\n",
      "   427 |                1 |            1 | 0.3226\n",
      "   428 |                1 |            1 | 0.3948\n",
      "   429 |                4 |            4 | 0.0853\n",
      "   430 |                0 |            0 | 0.0702\n",
      "   431 |                2 |            3 | 0.1810\n",
      "   432 |                1 |            1 | 0.1353\n",
      "   433 |                0 |            0 | 0.2077\n",
      "   434 |                4 |            4 | 0.2068\n",
      "   435 |                1 |            1 | 0.0912\n",
      "   436 |                3 |            2 | 0.1657\n",
      "   437 |                2 |            3 | 0.1864\n",
      "   438 |                0 |            0 | 0.2243\n",
      "   439 |                0 |            0 | 0.4337\n",
      "   440 |                2 |            3 | 0.1344\n",
      "   441 |                3 |            2 | 0.1798\n",
      "   442 |                2 |            3 | 0.1849\n",
      "   443 |                1 |            1 | 0.1798\n",
      "   444 |                0 |            0 | 0.1694\n",
      "   445 |                2 |            3 | 0.1786\n",
      "   446 |                4 |            4 | 0.1954\n",
      "   447 |                1 |            1 | 0.2175\n",
      "   448 |                0 |            0 | 0.1958\n",
      "   449 |                4 |            4 | 0.1032\n",
      "   450 |                3 |            2 | 0.1463\n",
      "   451 |                2 |            3 | 0.2040\n",
      "   452 |                1 |            1 | 0.2134\n",
      "   453 |                1 |            1 | 0.2371\n",
      "   454 |                1 |            1 | 0.2208\n",
      "   455 |                0 |            0 | 0.1665\n",
      "   456 |                1 |            1 | 0.2403\n",
      "   457 |                0 |            0 | 0.1744\n",
      "   458 |                3 |            2 | 0.2780\n",
      "   459 |                3 |            2 | 0.1453\n",
      "   460 |                1 |            1 | 0.1734\n",
      "   461 |                1 |            1 | 0.1939\n",
      "   462 |                3 |            2 | 0.1263\n",
      "   463 |                2 |            3 | 0.1644\n",
      "   464 |                1 |            1 | 0.2443\n",
      "   465 |                4 |            4 | 0.1564\n",
      "   466 |                0 |            0 | 0.1022\n",
      "   467 |                2 |            3 | 0.2104\n",
      "   468 |                1 |            1 | 0.0608\n",
      "   469 |                1 |            1 | 0.0284\n",
      "   470 |                0 |            0 | 0.3018\n",
      "   471 |                3 |            2 | 0.1047\n",
      "   472 |                4 |            4 | 0.1920\n",
      "   473 |                3 |            2 | 0.2845\n",
      "   474 |                3 |            2 | 0.1723\n",
      "   475 |                4 |            4 | 0.1438\n",
      "   476 |                0 |            0 | 0.0966\n",
      "   477 |                1 |            1 | 0.2359\n",
      "   478 |                2 |            3 | 0.1690\n",
      "   479 |                0 |            0 | 0.3154\n",
      "   480 |                3 |            2 | 0.2580\n",
      "   481 |                4 |            4 | 0.0822\n",
      "   482 |                3 |            2 | 0.2647\n",
      "   483 |                3 |            2 | 0.1660\n",
      "   484 |                3 |            2 | 0.0565\n",
      "   485 |                2 |            3 | 0.3524\n",
      "   486 |                4 |            4 | 0.1462\n",
      "   487 |                3 |            2 | 0.3263\n",
      "   488 |                4 |            4 | 0.2565\n",
      "   489 |                4 |            4 | 0.1023\n",
      "   490 |                3 |            2 | 0.2303\n",
      "   491 |                4 |            4 | 0.1000\n",
      "   492 |                1 |            1 | 0.0991\n",
      "   493 |                2 |            3 | 0.2295\n",
      "   494 |                3 |            2 | 0.1702\n",
      "   495 |                3 |            2 | 0.2086\n",
      "   496 |                2 |            3 | 0.1949\n",
      "   497 |                1 |            1 | 0.1061\n",
      "   498 |                0 |            0 | 0.2642\n",
      "   499 |                1 |            1 | 0.2323\n"
     ]
    }
   ],
   "source": [
    "# displaying the results \n",
    "print(\"Test Document Classification Results:\")\n",
    "print(\"Doc ID | Assigned Cluster | Label | Cosine Similarity\")\n",
    "for doc_id, cluster, label, similarity in test_results:\n",
    "    print(f\"{doc_id:6} | {cluster:16} | {label:12} | {similarity:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
