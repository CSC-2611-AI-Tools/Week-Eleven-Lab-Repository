{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba385b0a-d32f-4b5c-b007-cefa2d389bb3",
   "metadata": {},
   "source": [
    "# Document Clustering & Analysis using Custom K-Means\n",
    "This **Jupyter Notebook** looks a subset of the Newsgroup dataset. The **subset** for this dataset includes 2,500 documents, each belonging to one of **5 categories** which are: Windows (0), Crypt (1), Christian (2), Hockey (3), Forsale (4). The documents are represented by 9328 terms (stems). \n",
    "\n",
    "The **vocabulary** for the dataset is given in the file \"terms.txt\" and the **term-by-document** matrix is given in \"matrix.txt\". The actual category labels for the document is provided in the file \"classes.txt\". The **goal** of this lab is to perform clustering on the documents and compare the clusters to the actual categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab748407-4170-4e37-939c-d9b945cab643",
   "metadata": {},
   "source": [
    "### Importing Libraries\n",
    "We start by importing the following libraries:\n",
    "- **Pandas:** a powerful open-source data analysis and manipulation library for Python. It provides data structures and functions for efficiently working with structured data\n",
    "- **NumPy:** a open source Python library that's widely used in science and engineering. The NumPy library contains multidimensional array data structures, such as the homogeneous, N-dimensional ndarray\n",
    "- **Sklearn:**  a free and open-source machine learning library for the Python programming language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b667613-8afb-4a58-adb2-c8645ae73533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from kMeans import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8015b0-734f-4748-a540-99265ea11b7d",
   "metadata": {},
   "source": [
    "## 1 |  Cosine Distance Function\n",
    "We start off by creating the cosine distance function. We create our own distance function, where instead of using Euclidean distance, we use Cosine Similarity. This is the distance function that will be used to pass to the KMeans function in the Python Script. We use a written version that computes the Cosine Similarity between two n-dimensional vectors and returns the inverse as the distance between the vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5318000-3de4-4f0a-a86e-ae6c3a40bb6e",
   "metadata": {},
   "source": [
    "### Changes to the KMeans.py File\n",
    "Here is the following change that was made to the `kMeans.py` file where the Cosine Distance function was added with the following logic:\n",
    "```\n",
    "def distCosine(vecA, vecB):\n",
    "    vector_A = np.array(vecA)\n",
    "    vector_B = np.array(vecB)\n",
    "\n",
    "    dot_product = np.dot(vector_A, vector_B)\n",
    "    magnitude_A = np.linalg.norm(vector_A)\n",
    "    magnitude_B = np.linalg.norm(vector_B)\n",
    "\n",
    "    if magnitude_A == 0 or magnitude_B == 0:\n",
    "        return 1.0\n",
    "\n",
    "    cosine_similarity = dot_product / (magnitude_A * magnitude_B)\n",
    "    cosine_distance = 1 - cosine_similarity\n",
    "\n",
    "    return cosine_distance\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38f6bd2-8a86-4749-9311-ebbde23e3b31",
   "metadata": {},
   "source": [
    "## 2 | Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38c03e5f-6868-48ed-805e-191de8687ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the matrix data and transposing it\n",
    "data_matrix = np.loadtxt(\"matrix.txt\", delimiter=\",\")\n",
    "data_matrix = data_matrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "991f0d7f-dd2d-4119-9f9d-67a1d9c368b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset into training and testing dataset\n",
    "train_data, test_data = train_test_split(data_matrix, test_size=0.2, random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46b61349-d274-4970-90bf-e10b3f42927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing the TF-IDF transformation\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "train_data_tfidf = tfidf_transformer.fit_transform(train_data).toarray()\n",
    "test_data_tfidf = tfidf_transformer.transform(test_data).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183cf28e-9aac-40e5-8480-917ac38ad63c",
   "metadata": {},
   "source": [
    "## 2 | Performing KMeans Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e4d4958-c0d9-4285-bcef-fbfc84deca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('terms.txt', 'r') as file:\n",
    "    vocabulary = [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09faac2b-72a4-47a0-b9ba-9b550493b195",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "\n",
    "centroids, cluster_assment = kMeans(train_data_tfidf, k, distMeas=disCosine, createC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
